# Save the README content as a file

readme_content = """
# ğŸ“„ Chat with PDF using Groq

## ğŸ“‹ Overview

Interact with your PDF documents using a conversational AI powered by Groq's cutting-edge LLMs and Streamlit!  
Built for document understanding, question answering, and real-time knowledge extraction. ğŸš€

## âœ¨ Features

- **Multi-PDF Upload**  
  Upload one or several PDF documents at once for combined analysis. ğŸ“„
  
- **Dynamic Chunking**  
  Automatically adjusts the text split size based on document size for optimized retrieval. ğŸ”„

- **Embeddings-Based Search**  
  Create vector representations of document chunks using Sentence Transformers. ğŸ§ 

- **Groq Model Selection**  
  Choose from a wide range of high-performance Groq models for answering your queries. ğŸ¤–

- **First-Time Setup Auto-Install**  
  Automatically installs missing Python packages if they aren't detected. âš™ï¸

- **Streamlit Interface**  
  Clean, minimal, and interactive web UI for uploading files and chatting. ğŸ’»

- **Environment Configurable**  
  Manage API keys and settings easily via `.env` files. ğŸ”‘

## ğŸ”§ Technical Stack

### Backend
- **LangChain**: Document loading, splitting, retrieval, and QA chains. ğŸ”—
- **FAISS**: High-performance local vector store for fast similarity search. âš¡
- **Sentence Transformers**: Embedding generation via Hugging Face. ğŸ’¬
- **PyPDF2**: PDF parsing and text extraction. ğŸ“‘
- **Groq API**: Hosted LLM access for chat generation. ğŸŒ

### Frontend
- **Streamlit**: Real-time web UI for uploads, processing, and question answering. ğŸ–¥ï¸

### Infrastructure
- **Python-dotenv**: Environment variable management. ğŸ› ï¸
- **Subprocess / pip Auto-Installer**: First-time dependency management. ğŸ“¦

## ğŸš€ Getting Started

### 1. Clone the Repo  
```bash
git clone https://github.com/yourusername/chat-with-pdf-groq.git
cd chat-with-pdf-groq
```
### 2. Create & Activate Virtual Env  
```bash
python3 -m venv venv
source venv/bin/activate
``` 
### 3. Install Dependencies  
```bash
pip install -r requirements.txt
```
### 4. Set Environment Variables
```bash
Create a .env file in the project root with your Groq API key
GROQ_API_KEY=your-groq-api-key-here
```
### 5. Run the App
```bash
streamlit run app.py
```

## ğŸ“š Models Supported
- qwen-qwq-32b
- deepseek-r1-distill-llama-70b
- gemma2-9b-it
- compound-beta
- compound-beta-mini
- llama-3.1-8b-instant
- llama3-70b-8192
- meta-llama/llama-4-maverick-17b-128e-instruct
- mistral-saba-24b
- whisper-large-v3
- allam-2-7b
- and more... ğŸ§‘â€ğŸ’»

## ğŸ“ Project Structure
 
```
Multi-pdf-chatbot/
â”œâ”€â”€ .venv/                         # Virtual environment (dependencies, Python binaries)
â”‚   â”œâ”€â”€ etc/
â”‚   â”œâ”€â”€ Include/
â”‚   â”œâ”€â”€ Lib/
â”‚   â”œâ”€â”€ Scripts/
â”‚   â”œâ”€â”€ share/
â”‚   â””â”€â”€ pyvenv.cfg
â”œâ”€â”€ faiss_index/                   # Folder containing FAISS index files
â”‚   â”œâ”€â”€ index.faiss                # FAISS vector database
â”‚   â””â”€â”€ index.pkl                  # Metadata or mapping for the index
â”œâ”€â”€ .env                            # Environment variables configuration
â”œâ”€â”€ .packages_installed             # Installed packages record (optional)
â”œâ”€â”€ app.py                          # Main application script
â”œâ”€â”€ REQUIREMENTS.TXT                # Project dependencies
```


## ğŸ§  How It Works
1. **Upload PDFs**  
   Users upload PDFs through the sidebar. ğŸ“‚
2. **Text Extraction**  
   PyPDF2 extracts raw text from each page of the uploaded PDFs. ğŸ”
3. **Chunk Splitting**  
   Text is broken down using `RecursiveCharacterTextSplitter` based on document size. ğŸ§©
4. **Vector Embedding**  
   Chunks are converted to vector embeddings via a pre-trained HuggingFace model (`all-MiniLM-L6-v2`). ğŸ”¢
5. **FAISS Vector Store**  
   The embedded chunks are stored locally using FAISS for efficient retrieval. ğŸ’¾
6. **Question Input**  
   Users ask questions based on the uploaded documents. â“
7. **Contextual Retrieval**  
   FAISS fetches the top relevant document chunks using semantic similarity. ğŸƒâ€â™‚ï¸
8. **Answer Generation**  
   Retrieved context is passed into the selected Groq LLM using a custom prompt for answering. ğŸ§‘â€ğŸ«
9. **Streaming Results**  
   Answers are displayed back in real time on Streamlit. â±ï¸

## ğŸ› ï¸ Customization

- **Add New Groq Models**  
  Update the `AVAILABLE_MODELS` list in `app.py` with new model names. âš™ï¸
- **Change Embedding Models**  
  Modify the `get_embeddings()` function to swap in different Sentence Transformer models. ğŸ”„
- **Tune Prompt Templates**  
  Edit the `prompt_template` inside `make_qa_chain()` to guide Groq LLMs differently. âœï¸
- **Adjust Chunk Size Logic**  
  Tweak `compute_chunk_size()` if you want different splitting heuristics based on your dataset. ğŸ§³
  
## ğŸ“Š Performance Considerations
- **Chunk Size Tuning**  
  Large chunks may result in slower search but better context for answers. â³
- **Batch Inference**  
  Consider batching multiple user queries if scaling is needed. ğŸ—‚ï¸
- **GPU Deployment**  
  If you embed very large PDFs, running on GPU machines speeds up HuggingFace models. ğŸ–¥ï¸ğŸ’¨
- **Memory Management**  
  FAISS saves indexes locally, enabling persistence between runs. ğŸ’¾
- **Scaling Streamlit**  
  Deploy Streamlit via services like Streamlit Sharing, AWS EC2, or Hugging Face Spaces for multi-user access. ğŸŒ
  
## ğŸ¤ Contributing
Contributions are welcome! Please feel free to submit a Pull Request.
1. Fork the repository  
2. Create your feature branch (`git checkout -b feature/amazing-feature`)  
3. Commit your changes (`git commit -m 'Add some amazing feature'`)  
4. Push to the branch (`git push origin feature/amazing-feature`)  
5. Open a Pull Request  

## ğŸ“ License
This project is licensed under the **MIT License** â€“ see the [LICENSE](LICENSE) file for details. ğŸ“

